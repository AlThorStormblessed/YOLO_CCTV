{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting augmented dataset generation from face-annotated crops...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshg\\AppData\\Local\\Temp\\ipykernel_20876\\3045684339.py:43: UserWarning: Argument(s) 'var_limit' are not valid for transform GaussNoise\n",
      "  A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
      "Processing train:   3%|‚ñé         | 12/376 [00:10<05:31,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Agrim_Verma\\Agrim Verma _frame_13.txt: 0.885417, 0.704688, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:   5%|‚ñç         | 17/376 [00:15<05:21,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Agrim_Verma\\Agrim Verma _frame_2.txt: 0.69375, 0.657813, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  12%|‚ñà‚ñé        | 47/376 [00:42<04:42,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Ayush_patel\\Ayush Patel_frame_14.txt: 0.697917, 0.696875, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  24%|‚ñà‚ñà‚ñç       | 90/376 [01:19<04:07,  1.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Dhruv_singh\\Dhruv Singh_fullRight.txt: 0.647917, 0.654687, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  32%|‚ñà‚ñà‚ñà‚ñè      | 119/376 [01:45<03:45,  1.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Agrim_Verma\\Agrim Verma _frame_26.txt: 0.647917, 0.671094, 0.0, 0.067187\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 206/376 [02:59<02:22,  1.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Aumkumar_Savaliya\\Aumkumar Savaliya _center.txt: 0.66875, 0.607812, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 332/376 [04:45<00:36,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Inian_Aditya_M\\Inian Aditya M_halfLeftTop.txt: 0.660417, 0.679688, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 368/376 [05:15<00:06,  1.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Agrim_Verma\\Agrim Verma _frame_1.txt: 0.720833, 0.690625, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 376/376 [05:22<00:00,  1.17it/s]\n",
      "Processing val:  32%|‚ñà‚ñà‚ñà‚ñè      | 30/95 [00:25<00:57,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è Skipping invalid bbox in yolo_annotated_images/Amit_Kumar_Meena\\Amit kumar meena _frame_8.txt: 0.841667, 0.704688, 0.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 95/95 [01:20<00:00,  1.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All done with augmentation and label transformation!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import albumentations as A\n",
    "# from albumentations.augmentations.dropout import Cutout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"Starting augmented dataset generation from face-annotated crops...\")\n",
    "\n",
    "input_root = \"yolo_annotated_images/\"\n",
    "base_output_path = \"yolo_annotated_images/yolo_dataset\"\n",
    "\n",
    "output_dirs = {\n",
    "    \"train_img\": os.path.join(base_output_path, \"images/train\"),\n",
    "    \"train_lbl\": os.path.join(base_output_path, \"labels/train\"),\n",
    "    \"val_img\": os.path.join(base_output_path, \"images/val\"),\n",
    "    \"val_lbl\": os.path.join(base_output_path, \"labels/val\")\n",
    "}\n",
    "for path in output_dirs.values():\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "transform = A.Compose([\n",
    "    A.Resize(640, 640),\n",
    "    \n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ShiftScaleRotate(\n",
    "        shift_limit=0.1, scale_limit=0.2, rotate_limit=20, p=0.7, border_mode=0\n",
    "    ),\n",
    "\n",
    "    A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.5),\n",
    "    A.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.2, p=0.5),\n",
    "    A.RGBShift(r_shift_limit=25, g_shift_limit=25, b_shift_limit=25, p=0.3),\n",
    "    A.HueSaturationValue(hue_shift_limit=20, sat_shift_limit=30, val_shift_limit=20, p=0.3),\n",
    "\n",
    "    A.MotionBlur(blur_limit=5, p=0.2),\n",
    "    A.GaussianBlur(blur_limit=3, p=0.2),\n",
    "    A.GaussNoise(var_limit=(10.0, 50.0), p=0.3),\n",
    "\n",
    "    \n",
    "    A.Perspective(scale=(0.05, 0.1), p=0.2)\n",
    "], bbox_params=A.BboxParams(format='yolo', label_fields=['class_labels']))\n",
    "\n",
    "person_folders = sorted(os.listdir(input_root))\n",
    "class_map = {name: idx for idx, name in enumerate(person_folders)}\n",
    "\n",
    "os.makedirs(base_output_path, exist_ok=True)\n",
    "with open(os.path.join(base_output_path, \"classes.txt\"), \"w\") as f:\n",
    "    for name, idx in class_map.items():\n",
    "        f.write(f\"{idx} {name}\\n\")\n",
    "\n",
    "all_image_label_pairs = []\n",
    "for person in person_folders:\n",
    "    person_path = os.path.join(input_root, person)\n",
    "    for img_file in os.listdir(person_path):\n",
    "        if img_file.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
    "            img_path = os.path.join(person_path, img_file)\n",
    "            label_path = os.path.splitext(img_path)[0] + \".txt\"\n",
    "            if os.path.exists(label_path):\n",
    "                all_image_label_pairs.append((img_path, label_path, person))\n",
    "\n",
    "train_pairs, val_pairs = train_test_split(all_image_label_pairs, test_size=0.2, random_state=42)\n",
    "all_pairs = {\"train\": train_pairs, \"val\": val_pairs}\n",
    "\n",
    "for split, pairs in all_pairs.items():\n",
    "    for img_path, label_path, person in tqdm(pairs, desc=f\"Processing {split}\"):\n",
    "        class_id = class_map[person]\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            print(f\"Cannot read {img_path}\")\n",
    "            continue\n",
    "\n",
    "        with open(label_path, \"r\") as f:\n",
    "            boxes = []\n",
    "            class_labels = []\n",
    "            for line in f.readlines():\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) != 5:\n",
    "                    continue\n",
    "                _, x, y, bw, bh = map(float, parts)\n",
    "\n",
    "                if bw <= 0 or bh <= 0 or x <= 0 or y <= 0:\n",
    "                    print(f\"Skipping invalid bbox in {label_path}: {x}, {y}, {bw}, {bh}\")\n",
    "                    continue\n",
    "\n",
    "                boxes.append([x, y, bw, bh])\n",
    "                class_labels.append(class_id)\n",
    "\n",
    "        for i in range(40):\n",
    "            base_name = f\"{person}_{os.path.splitext(os.path.basename(img_path))[0]}_{i}.jpg\"\n",
    "            out_img_path = os.path.join(output_dirs[f\"{split}_img\"], base_name)\n",
    "            out_lbl_path = os.path.join(output_dirs[f\"{split}_lbl\"], base_name.replace(\".jpg\", \".txt\"))\n",
    "\n",
    "            if os.path.exists(out_img_path) and os.path.exists(out_lbl_path):\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                augmented = transform(image=img, bboxes=boxes, class_labels=class_labels)\n",
    "            except Exception as e:\n",
    "                print(f\"Augmentation failed on {img_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "            aug_img = augmented[\"image\"]\n",
    "            aug_boxes = augmented[\"bboxes\"]\n",
    "            aug_labels = augmented[\"class_labels\"]\n",
    "\n",
    "            cv2.imwrite(out_img_path, aug_img)\n",
    "\n",
    "            with open(out_lbl_path, \"w\") as f:\n",
    "                for label, bbox in zip(aug_labels, aug_boxes):\n",
    "                    x, y, bw, bh = bbox\n",
    "                    f.write(f\"{label} {x:.6f} {y:.6f} {bw:.6f} {bh:.6f}\\n\")\n",
    "\n",
    "print(\"All done with augmentation and label transformation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Aabis',\n",
       " 'Aaditya_Kumawat',\n",
       " 'Agrim_Verma',\n",
       " 'Ak',\n",
       " 'Akshat_Sharma',\n",
       " 'Amit_Kumar_Meena',\n",
       " 'Ananta',\n",
       " 'Aumkumar_Savaliya',\n",
       " 'Avanish',\n",
       " 'Ayush_Chandra',\n",
       " 'Ayush_patel',\n",
       " 'Dhruv_singh',\n",
       " 'Gorang_Rathi',\n",
       " 'Hrishikesh_Giri',\n",
       " 'Inian_Aditya_M',\n",
       " 'Kartik_Sharma',\n",
       " 'Krishang_Goyal',\n",
       " 'Krish_Bansal',\n",
       " 'Miling_soni',\n",
       " 'Param',\n",
       " 'Yash',\n",
       " 'yolo_dataset']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "[i[22:] for i in glob.glob(\"yolo_annotated_images/*\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new Ultralytics Settings v0.0.6 file ‚úÖ \n",
      "View Ultralytics Settings with 'yolo settings' or at 'C:\\Users\\anshg\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n",
      "Downloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolov8n.pt to 'yolov8n.pt'...\n",
      "Ultralytics 8.3.111 üöÄ Python-3.12.3 torch-2.6.0+cpu CPU (AMD Ryzen 7 6800H with Radeon Graphics)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=dataset.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anshg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\_pytree.py:185: FutureWarning: optree is installed but the version is too old to support PyTorch Dynamo in C++ pytree. C++ pytree support is disabled. Please consider upgrading optree using `python3 -m pip install --upgrade 'optree>=0.13.0'`.\n",
      "  warnings.warn(\n",
      "\n",
      "  0%|          | 0.00/6.25M [00:00<?, ?B/s]\n",
      "  2%|‚ñè         | 128k/6.25M [00:00<00:07, 829kB/s]\n",
      "  4%|‚ñç         | 256k/6.25M [00:00<00:07, 882kB/s]\n",
      "  6%|‚ñå         | 384k/6.25M [00:00<00:06, 923kB/s]\n",
      "  8%|‚ñä         | 512k/6.25M [00:00<00:06, 919kB/s]\n",
      " 10%|‚ñà         | 640k/6.25M [00:00<00:06, 952kB/s]\n",
      " 12%|‚ñà‚ñè        | 768k/6.25M [00:00<00:06, 918kB/s]\n",
      " 14%|‚ñà‚ñç        | 896k/6.25M [00:01<00:06, 867kB/s]\n",
      " 16%|‚ñà‚ñå        | 1.00M/6.25M [00:01<00:06, 887kB/s]\n",
      " 18%|‚ñà‚ñä        | 1.12M/6.25M [00:01<00:05, 908kB/s]\n",
      " 20%|‚ñà‚ñà        | 1.25M/6.25M [00:01<00:05, 907kB/s]\n",
      " 22%|‚ñà‚ñà‚ñè       | 1.38M/6.25M [00:01<00:05, 960kB/s]\n",
      " 24%|‚ñà‚ñà‚ñç       | 1.50M/6.25M [00:01<00:04, 1.02MB/s]\n",
      " 26%|‚ñà‚ñà‚ñå       | 1.62M/6.25M [00:01<00:04, 1.05MB/s]\n",
      " 28%|‚ñà‚ñà‚ñä       | 1.75M/6.25M [00:01<00:05, 929kB/s] \n",
      " 30%|‚ñà‚ñà‚ñà       | 1.88M/6.25M [00:02<00:04, 923kB/s]\n",
      " 32%|‚ñà‚ñà‚ñà‚ñè      | 2.00M/6.25M [00:02<00:04, 929kB/s]\n",
      " 34%|‚ñà‚ñà‚ñà‚ñç      | 2.12M/6.25M [00:02<00:04, 931kB/s]\n",
      " 36%|‚ñà‚ñà‚ñà‚ñå      | 2.25M/6.25M [00:02<00:04, 990kB/s]\n",
      " 38%|‚ñà‚ñà‚ñà‚ñä      | 2.38M/6.25M [00:02<00:03, 1.03MB/s]\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 2.50M/6.25M [00:02<00:03, 1.08MB/s]\n",
      " 42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 2.62M/6.25M [00:02<00:03, 1.12MB/s]\n",
      " 46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 2.88M/6.25M [00:03<00:03, 1.16MB/s]\n",
      " 48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 3.00M/6.25M [00:03<00:02, 1.18MB/s]\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 3.12M/6.25M [00:03<00:02, 1.11MB/s]\n",
      " 54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 3.38M/6.25M [00:03<00:02, 1.18MB/s]\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 3.50M/6.25M [00:03<00:02, 1.17MB/s]\n",
      " 58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 3.62M/6.25M [00:03<00:02, 1.18MB/s]\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3.75M/6.25M [00:03<00:02, 1.20MB/s]\n",
      " 62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 3.88M/6.25M [00:03<00:02, 1.21MB/s]\n",
      " 66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 4.12M/6.25M [00:04<00:01, 1.27MB/s]\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 4.38M/6.25M [00:04<00:01, 1.35MB/s]\n",
      " 74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 4.62M/6.25M [00:04<00:01, 1.42MB/s]\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 4.88M/6.25M [00:04<00:00, 1.46MB/s]\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 5.12M/6.25M [00:04<00:00, 1.55MB/s]\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 5.38M/6.25M [00:05<00:00, 1.43MB/s]\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 5.62M/6.25M [00:05<00:00, 1.53MB/s]\n",
      " 94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 5.88M/6.25M [00:05<00:00, 1.64MB/s]\n",
      " 98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 6.12M/6.25M [00:05<00:00, 1.72MB/s]\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6.25M/6.25M [00:05<00:00, 1.19MB/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\anshg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py\", line 582, in get_dataset\n",
      "    data = check_det_dataset(self.args.data)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\anshg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\data\\utils.py\", line 452, in check_det_dataset\n",
      "    raise FileNotFoundError(m)\n",
      "FileNotFoundError: \n",
      "Dataset 'dataset.yaml' images not found ‚ö†Ô∏è, missing path 'C:\\Users\\anshg\\Python_shit\\ML\\YOLO\\datasets\\yolo_dataset\\images\\train'\n",
      "Note dataset download directory is 'C:\\Users\\anshg\\Python_shit\\ML\\YOLO\\datasets'. You can update this in 'C:\\Users\\anshg\\AppData\\Roaming\\Ultralytics\\settings.json'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"C:\\Users\\anshg\\AppData\\Local\\Programs\\Python\\Python312\\Scripts\\yolo.exe\\__main__.py\", line 7, in <module>\n",
      "  File \"C:\\Users\\anshg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\cfg\\__init__.py\", line 989, in entrypoint\n",
      "    getattr(model, mode)(**overrides)  # default args from model\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\anshg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\model.py\", line 785, in train\n",
      "    self.trainer = (trainer or self._smart_load(\"trainer\"))(overrides=args, _callbacks=self.callbacks)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\anshg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py\", line 137, in __init__\n",
      "    self.trainset, self.testset = self.get_dataset()\n",
      "                                  ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\anshg\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\ultralytics\\engine\\trainer.py\", line 586, in get_dataset\n",
      "    raise RuntimeError(emojis(f\"Dataset '{clean_url(self.args.data)}' error ‚ùå {e}\")) from e\n",
      "RuntimeError: Dataset 'dataset.yaml' error  \n",
      "Dataset 'dataset.yaml' images not found , missing path 'C:\\Users\\anshg\\Python_shit\\ML\\YOLO\\datasets\\yolo_dataset\\images\\train'\n",
      "Note dataset download directory is 'C:\\Users\\anshg\\Python_shit\\ML\\YOLO\\datasets'. You can update this in 'C:\\Users\\anshg\\AppData\\Roaming\\Ultralytics\\settings.json'\n"
     ]
    }
   ],
   "source": [
    "!yolo detect train model=yolov8n.pt data=dataset.yaml epochs=50 imgsz=640\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
